{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abd0912e",
   "metadata": {},
   "source": [
    "# Chains\n",
    "1. Sequential Chain\n",
    "2. Parallel Chain\n",
    "3. Conditional Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975278f0",
   "metadata": {},
   "source": [
    "## Sequential Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1dfd50",
   "metadata": {},
   "source": [
    "Alright Stark, let’s dive into **Chains in LangChain** with your customized teaching style.\n",
    "\n",
    "=================================================\n",
    "Chains in LangChain\n",
    "===================\n",
    "\n",
    "1. Introduction / Definition\n",
    "   A chain in LangChain is a sequence of steps or components that connect together to process inputs and produce outputs. Instead of calling a large language model (LLM) directly, chains let you combine LLMs with prompts, output parsers, tools, or even multiple LLM calls. Think of it as a workflow manager where each step is clearly defined.\n",
    "\n",
    "2. Why it is Important\n",
    "\n",
    "* LLMs alone only answer a single prompt, but real-world tasks often need multiple coordinated steps.\n",
    "* Chains provide structure, reusability, and flexibility to build advanced applications (like chatbots, reasoning pipelines, or document analyzers).\n",
    "* They help enforce consistency — for example, always format the output as JSON, or always summarize after extracting key points.\n",
    "\n",
    "3. How it Works (intuitive explanation)\n",
    "   Imagine you are preparing for an exam. You don’t just “read the textbook once and write the answers.” You:\n",
    "   a) Read the question\n",
    "   b) Recall the right topic\n",
    "   c) Summarize your understanding\n",
    "   d) Write the structured answer\n",
    "\n",
    "Similarly, a chain takes input, passes it through different components (like prompt templates, models, or parsers), and delivers the final structured output. Each part can add value — like cleaning input, formatting prompts, or validating output.\n",
    "\n",
    "4. Types or Variants\n",
    "   There are several common types of chains in LangChain:\n",
    "\n",
    "a) LLMChain\n",
    "\n",
    "* The simplest chain: input → prompt → LLM → output.\n",
    "* Example: Summarize a document in one step.\n",
    "\n",
    "b) SequentialChain\n",
    "\n",
    "* Runs multiple chains one after the other.\n",
    "* Example: First summarize a document, then translate the summary.\n",
    "\n",
    "c) SimpleSequentialChain\n",
    "\n",
    "* A lightweight sequential chain where the output of one chain is directly fed into the next.\n",
    "\n",
    "d) RouterChain\n",
    "\n",
    "* Routes input to different sub-chains depending on conditions.\n",
    "* Example: If input language is English → summarize. If input is French → translate first.\n",
    "\n",
    "e) RetrievalQA Chain\n",
    "\n",
    "* Combines LLMs with retrieval (from a database or vector store).\n",
    "* Example: Ask questions over your documents.\n",
    "\n",
    "f) API Chains / Tool Chains\n",
    "\n",
    "* Allow LLMs to call external APIs/tools as part of reasoning.\n",
    "* Example: An LLM that queries a weather API before answering.\n",
    "\n",
    "5. Code Examples (Python)\n",
    "\n",
    "Basic LLMChain:\n",
    "\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI  \n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")  \n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"movie\"],\n",
    "    template=\"Write a short review for the movie {movie}.\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "result = chain.run(\"Inception\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "SequentialChain:\n",
    "\n",
    "```python\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# First chain: summarize\n",
    "chain1 = LLMChain(llm=llm, prompt=PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Summarize the following text:\\n{text}\"\n",
    "))\n",
    "\n",
    "# Second chain: sentiment\n",
    "chain2 = LLMChain(llm=llm, prompt=PromptTemplate(\n",
    "    input_variables=[\"summary\"],\n",
    "    template=\"What is the sentiment of this summary?\\n{summary}\"\n",
    "))\n",
    "\n",
    "overall_chain = SequentialChain(chains=[chain1, chain2], input_variables=[\"text\"])\n",
    "result = overall_chain.run({\"text\": \"The movie was great but a bit long.\"})\n",
    "print(result)\n",
    "```\n",
    "\n",
    "6. Real-life Analogies\n",
    "   Think of chains like a **restaurant kitchen workflow**.\n",
    "\n",
    "* Order comes in (input).\n",
    "* First chef chops vegetables (prompting).\n",
    "* Second chef cooks them (LLM).\n",
    "* Third chef plates the food (output parser).\n",
    "* Final dish goes to the customer (structured output).\n",
    "  Without chains, the chef would have to do everything unstructured. Chains enforce a pipeline where each step is handled properly.\n",
    "\n",
    "7. Applications / Use cases\n",
    "\n",
    "* Chatbots that answer in structured formats (FAQ → JSON).\n",
    "* Document analysis (summarization + sentiment + keyword extraction).\n",
    "* Multi-step reasoning tasks (research assistant, code generation + execution).\n",
    "* Tool-augmented LLMs (calculator, search engine, weather API).\n",
    "* Routing complex inputs to the right model (translation chain vs summarization chain).\n",
    "\n",
    "8. Advantages and Limitations\n",
    "\n",
    "Advantages:\n",
    "\n",
    "* Modular and reusable (you can swap components easily).\n",
    "* Ensures structured workflows.\n",
    "* Reduces prompt-engineering headaches by separating concerns.\n",
    "* Makes applications robust (with parsers, validation, and fallback logic).\n",
    "\n",
    "Limitations:\n",
    "\n",
    "* More complex than using the LLM directly.\n",
    "* Performance overhead if many chains are used.\n",
    "* Requires careful design (bad chaining can confuse the model).\n",
    "\n",
    "9. Final Summary with Key Points\n",
    "\n",
    "* Chains in LangChain = workflows that connect LLMs, prompts, and other components.\n",
    "* Useful for tasks that need multiple steps or structured outputs.\n",
    "* Main types include LLMChain, SequentialChain, SimpleSequentialChain, RouterChain, and RetrievalQA.\n",
    "* They make LLM-based apps more **organized, reliable, and scalable**.\n",
    "* However, they add complexity and should be used thoughtfully.\n",
    "\n",
    "=================================================\n",
    "\n",
    "Would you like me to also make a **comparison table of all chain types with their input/output style** so you can paste that into your notes for quick reference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9faada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ac3d015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758799468.336805   40982 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are five facts about India:\n",
      "\n",
      "1.  **Most Populous Country:** India is the **most populous country in the world**, having surpassed China in 2023. Its population is over 1.4 billion people.\n",
      "2.  **Incredible Diversity:** It is an extremely **diverse nation** with over 22 official languages (and hundreds of dialects), a multitude of religions (Hinduism, Islam, Christianity, Sikhism, Buddhism, Jainism, etc.), and a rich tapestry of cultures, traditions, and festivals.\n",
      "3.  **Historical Inventions:** India is credited with the **invention of the numeral zero**, the decimal system, and the game of chess. Yoga and Ayurveda (an ancient system of medicine) also originated here.\n",
      "4.  **Seventh-Largest Country:** India is the **seventh-largest country by land area** in the world, covering a vast geographical range that includes the Himalayan mountains, deserts, fertile plains, and a long coastline.\n",
      "5.  **Largest Film Industry (Bollywood):** India is home to the **world's largest film industry**, known as Bollywood, which produces thousands of films annually and has a massive global audience. It's also a major global hub for information technology and pharmaceuticals.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "model = GoogleGenerativeAI(model = 'gemini-2.5-flash')\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template= \"Give me the five facts of the {topic}\",\n",
    "    input_variables=['topic']\n",
    "    \n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = template | model | parser\n",
    "\n",
    "result = chain.invoke({'topic':'india'})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a45c88b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "  +--------------------+   \n",
      "  | GoogleGenerativeAI |   \n",
      "  +--------------------+   \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "graph = chain.get_graph()\n",
    "graph.print_ascii()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf81ada3",
   "metadata": {},
   "source": [
    "### Complex Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12e4e809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758796428.110737   40982 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 key points about LoRA and QLoRA from the provided text:\n",
      "\n",
      "1.  **Core Purpose: Revolutionizing Fine-tuning.** Both LoRA and QLoRA dramatically reduce the computational and memory requirements for fine-tuning large pre-trained models (like LLMs), making the process more accessible and efficient than traditional full fine-tuning.\n",
      "2.  **LoRA's Mechanism: Low-Rank Adapters.** LoRA works by freezing the vast majority of the pre-trained model's weights and instead injecting a small number of trainable \"low-rank adaptation\" matrices (A and B) into specific layers (typically attention Q/V projection matrices). Only these tiny adapter matrices are updated during training.\n",
      "3.  **LoRA's Key Benefits: Efficiency & Small Footprint.** This approach leads to significant memory efficiency during training (updating a fraction of parameters), faster training convergence, and results in extremely small \"LoRA adapters\" (a few MBs) that are easy to store, share, and dynamically switch to adapt a single base model to multiple tasks.\n",
      "4.  **QLoRA's Innovation: Quantized Base Model.** QLoRA extends LoRA by quantizing the entire pre-trained base model to 4-bit precision, specifically using the 4-bit NormalFloat (NF4) data type. This drastically reduces the base model's memory footprint in VRAM, even before training begins.\n",
      "5.  **QLoRA's Impact: Unprecedented Accessibility.** This 4-bit quantization, combined with techniques like double quantization and paged optimizers, results in massive VRAM savings (up to 3x compared to LoRA). It allows fine-tuning of models previously inaccessible to consumer hardware (e.g., 65B+ models on a single 48GB GPU), while still performing computations (by dequantizing on the fly) and training LoRA adapters in higher precision to preserve model quality.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model = GoogleGenerativeAI(model = 'gemini-2.5-flash')\n",
    "\n",
    "template1 = PromptTemplate(\n",
    "    template= \"Describe about the {topic}\",\n",
    "    input_variables=['topic']\n",
    "    \n",
    ")\n",
    "template2 = PromptTemplate(\n",
    "    template=\"Give 5 points from the \\n {text}\",\n",
    "    input_variables=['text']\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = template1 | model | parser | template2 | model | parser\n",
    "\n",
    "result = chain.invoke({'topic':'Lora  and QLora'})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edd52ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "  +--------------------+   \n",
      "  | GoogleGenerativeAI |   \n",
      "  +--------------------+   \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "  +--------------------+   \n",
      "  | GoogleGenerativeAI |   \n",
      "  +--------------------+   \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "graph = chain.get_graph()\n",
    "graph.print_ascii()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17236b08",
   "metadata": {},
   "source": [
    "## Parallel Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac9586e",
   "metadata": {},
   "source": [
    "Alright Stark, let’s carefully go over **Parallel Chains in LangChain** with your customized style (class-notes style).\n",
    "\n",
    "=================================================\n",
    "Parallel Chains in LangChain\n",
    "============================\n",
    "\n",
    "1. Introduction / Definition\n",
    "   A **Parallel Chain** in LangChain is a way to run multiple chains (or components) **at the same time** on the same input.\n",
    "\n",
    "* Instead of processing one step after another (like in Sequential Chains), parallel chains **fan out** the input into multiple branches, run them concurrently, and then collect the results together.\n",
    "* The final output is a dictionary with each branch’s result.\n",
    "\n",
    "2. Why it is Important\n",
    "\n",
    "* Efficiency: Useful when you want to perform multiple tasks on the same input without waiting for one to finish before starting another.\n",
    "* Reusability: Each branch can be a simple chain on its own, but when grouped in parallel, they produce a richer result.\n",
    "* Organization: Keeps logic neat when one input must produce multiple kinds of outputs (e.g., summary, sentiment, keywords).\n",
    "\n",
    "3. How it Works (intuitive explanation)\n",
    "   Imagine you are a teacher giving one essay to three students.\n",
    "\n",
    "* Student A writes a **summary**.\n",
    "* Student B identifies **keywords**.\n",
    "* Student C writes the **sentiment analysis**.\n",
    "  All three work at the same time, and finally, you collect their answers into one notebook.\n",
    "\n",
    "That’s exactly what a parallel chain does: one input → multiple independent branches → combined dictionary of results.\n",
    "\n",
    "4. Types or Variants\n",
    "   In LangChain, parallel execution is supported via:\n",
    "\n",
    "* `RunnableParallel` → runs multiple runnables (chains, prompts, models) in parallel and returns their results as a dictionary.\n",
    "* It’s not a separate \"Chain class\" like SequentialChain but a composition built using the LangChain Expression Language (LCEL).\n",
    "\n",
    "5. Code Examples\n",
    "\n",
    "Example 1: Using RunnableParallel\n",
    "\n",
    "```python\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = GoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Prompt 1: summary\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Summarize the following text:\\n{text}\"\n",
    ")\n",
    "\n",
    "# Prompt 2: sentiment\n",
    "sentiment_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"What is the sentiment (positive/negative/neutral) of this text:\\n{text}\"\n",
    ")\n",
    "\n",
    "# Prompt 3: keywords\n",
    "keywords_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Extract 5 important keywords from this text:\\n{text}\"\n",
    ")\n",
    "\n",
    "# Chains for each task\n",
    "summary_chain = summary_prompt | model | parser\n",
    "sentiment_chain = sentiment_prompt | model | parser\n",
    "keywords_chain = keywords_prompt | model | parser\n",
    "\n",
    "# Parallel composition\n",
    "parallel_chain = RunnableParallel(\n",
    "    summary=summary_chain,\n",
    "    sentiment=sentiment_chain,\n",
    "    keywords=keywords_chain\n",
    ")\n",
    "\n",
    "text_input = \"LoRa and QLoRa are efficient techniques to fine-tune large models with fewer resources.\"\n",
    "\n",
    "result = parallel_chain.invoke({\"text\": text_input})\n",
    "print(result)\n",
    "```\n",
    "\n",
    "Expected output (dictionary):\n",
    "\n",
    "```python\n",
    "{\n",
    "  'summary': 'LoRa and QLoRa are methods for efficient fine-tuning...',\n",
    "  'sentiment': 'Positive',\n",
    "  'keywords': 'LoRa, QLoRa, fine-tuning, efficiency, resources'\n",
    "}\n",
    "```\n",
    "\n",
    "6. Real-life Analogies\n",
    "\n",
    "* Think of **parallel chains** like ordering a meal combo: the kitchen makes your burger, fries, and drink at the same time instead of one after the other.\n",
    "* Or like running multiple medical tests on one blood sample: lab A checks sugar, lab B checks cholesterol, lab C checks hemoglobin, and you get a combined report.\n",
    "\n",
    "7. Applications / Use cases\n",
    "\n",
    "* Text analysis: From one input text, get summary, sentiment, keywords, and rating simultaneously.\n",
    "* Multi-perspective answers: Generate a creative explanation + a technical explanation at the same time.\n",
    "* Question answering: Get both a **direct answer** and a **supporting passage** in parallel.\n",
    "* Content moderation: From one user post, run multiple safety/violation checks in parallel.\n",
    "\n",
    "8. Advantages and Limitations\n",
    "\n",
    "Advantages:\n",
    "\n",
    "* Saves time by running tasks concurrently.\n",
    "* Cleaner code: No need to manually orchestrate multiple chains and merge results.\n",
    "* Scalable: Can easily add or remove branches.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "* If branches are very heavy, actual concurrency might depend on the backend (some models/hosts still process sequentially).\n",
    "* Complexity increases if outputs need to be combined in specific ways (parallel only merges, it doesn’t chain outputs).\n",
    "* Error handling: If one branch fails, you need to decide whether to fail all or continue with partial results.\n",
    "\n",
    "9. Final Summary with Key Points\n",
    "\n",
    "* Parallel chains let you run multiple chains on the same input simultaneously.\n",
    "* Best for situations where one input must produce multiple different outputs (summary, sentiment, keywords, etc.).\n",
    "* Implemented using `RunnableParallel` in LangChain Expression Language.\n",
    "* Analogy: One essay, multiple students give different outputs in parallel.\n",
    "* Advantage: efficiency, clean multi-output pipelines. Limitation: may complicate error handling and still depend on backend concurrency.\n",
    "\n",
    "=================================================\n",
    "\n",
    "Would you like me to also show a **comparison between Sequential Chains vs Parallel Chains** in a side-by-side table for your notes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0644da77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758800029.493329   40982 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': 'LoRa and QLoRa are efficient techniques used to fine-tune large models while requiring fewer resources.', 'sentiment': 'The sentiment of the text is **positive**.\\n\\nIt uses words like \"efficient\" and phrases like \"with fewer resources,\" which describe beneficial and desirable qualities.', 'keywords': 'Here are 5 important keywords from the text:\\n\\n1.  **LoRa**\\n2.  **QLoRa**\\n3.  **fine-tune**\\n4.  **large models**\\n5.  **fewer resources**'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = GoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Prompt 1: summary\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Summarize the following text:\\n{text}\"\n",
    ")\n",
    "\n",
    "# Prompt 2: sentiment\n",
    "sentiment_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"What is the sentiment (positive/negative/neutral) of this text:\\n{text}\"\n",
    ")\n",
    "\n",
    "# Prompt 3: keywords\n",
    "keywords_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Extract 5 important keywords from this text:\\n{text}\"\n",
    ")\n",
    "\n",
    "# Chains for each task\n",
    "summary_chain = summary_prompt | model | parser\n",
    "sentiment_chain = sentiment_prompt | model | parser\n",
    "keywords_chain = keywords_prompt | model | parser\n",
    "\n",
    "# Parallel composition\n",
    "parallel_chain = RunnableParallel(\n",
    "    summary=summary_chain,\n",
    "    sentiment=sentiment_chain,\n",
    "    keywords=keywords_chain\n",
    ")\n",
    "\n",
    "text_input = \"LoRa and QLoRa are efficient techniques to fine-tune large models with fewer resources.\"\n",
    "\n",
    "result = parallel_chain.invoke({\"text\": text_input})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36721631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758803187.826718   40982 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the merged document containing detailed notes on LoRA and QLoRA, followed by a quiz to test your understanding.\n",
      "\n",
      "---\n",
      "\n",
      "## Detailed Notes on LoRA (Low-Rank Adaptation of Large Language Models)\n",
      "\n",
      "### 1. Introduction & Core Idea\n",
      "\n",
      "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning (PEFT) technique introduced in 2021 by Microsoft. Its core idea is to significantly reduce the number of trainable parameters during fine-tuning of large pre-trained models (like LLMs or Diffusion Models) by injecting small, low-rank matrices into the existing model architecture. Instead of fine-tuning all original weights, only these injected matrices are trained.\n",
      "\n",
      "### 2. Motivation: Why LoRA was Developed\n",
      "\n",
      "Large pre-trained models (e.g., GPT-3, Llama, Stable Diffusion) have billions of parameters. Fine-tuning these models for specific downstream tasks traditionally involves updating *all* these parameters. This comes with significant challenges:\n",
      "\n",
      "*   **Computational Cost:** Requires immense GPU memory and processing power.\n",
      "*   **Storage Cost:** Storing a full fine-tuned checkpoint for each task is prohibitive. A 175B parameter model might require hundreds of GBs per checkpoint.\n",
      "*   **Catastrophic Forgetting:** Full fine-tuning can sometimes lead to the model \"forgetting\" general knowledge learned during pre-training.\n",
      "\n",
      "LoRA addresses these issues by offering a more efficient alternative.\n",
      "\n",
      "### 3. How LoRA Works (Technical Details)\n",
      "\n",
      "1.  **Identify Target Layers:** LoRA is typically applied to the weight matrices of specific layers, most commonly the attention mechanism's query (Q), key (K), value (V), and output (O) projection matrices in transformers. It can also be applied to feed-forward layers.\n",
      "2.  **Decomposition of Weight Updates:** For a pre-trained weight matrix $W_0 \\in \\mathbb{R}^{d \\times k}$ (e.g., a query projection matrix), standard fine-tuning would update it to $W_0 + \\Delta W$. LoRA proposes to represent this update $\\Delta W$ using a low-rank decomposition:\n",
      "    $$ \\Delta W = B A $$\n",
      "    where $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times k}$. Here, $r$ is the \"rank\" of the adaptation, and $r \\ll \\min(d, k)$.\n",
      "3.  **Injected Adapters:** Instead of directly modifying $W_0$, LoRA injects these two smaller matrices, $A$ and $B$, into the model. During inference, the output of a layer becomes $(W_0 + BA)x$.\n",
      "4.  **Training Process:**\n",
      "    *   The original pre-trained weight matrix $W_0$ is *frozen* (its gradients are not computed, and it is not updated).\n",
      "    *   Only the newly introduced matrices $A$ and $B$ are trainable parameters.\n",
      "    *   $A$ is typically initialized with random Gaussian values, and $B$ is initialized with zeros. This ensures that $BA$ is initially zero, so the fine-tuning process starts with the pre-trained model's original behavior.\n",
      "5.  **Scaling:** A scaling factor $\\alpha/r$ is often used, where $\\alpha$ is a constant (e.g., 16 or 32) and $r$ is the rank. The final update becomes $(\\alpha/r)BA$. This scaling helps control the magnitude of the updates and can improve performance.\n",
      "\n",
      "### 4. Key Features & Parameters\n",
      "\n",
      "*   **Rank ($r$):** The most critical hyperparameter. A smaller $r$ means fewer trainable parameters but potentially less expressive power. Typical values range from 4 to 64.\n",
      "*   **Alpha ($\\alpha$):** The scaling factor. Often set equal to $r$ or higher.\n",
      "*   **Target Layers:** Which layers to apply LoRA to (e.g., `q_proj`, `k_proj`, `v_proj`, `o_proj`).\n",
      "*   **Frozen Base Model:** The core pre-trained model weights remain unchanged.\n",
      "\n",
      "### 5. Benefits of LoRA\n",
      "\n",
      "*   **Massive Reduction in Trainable Parameters:** Instead of millions/billions, only thousands or a few million parameters are trained. This makes fine-tuning feasible on consumer-grade GPUs.\n",
      "*   **Reduced Memory Footprint:** Less memory is needed for optimizer states and gradients.\n",
      "*   **Faster Training:** Fewer parameters to update means faster gradient computation and backpropagation.\n",
      "*   **Smaller Checkpoints:** A LoRA adapter is typically just a few MBs (e.g., 5-20 MB for a 7B LLM), making it easy to store and share multiple fine-tuned versions.\n",
      "*   **Composability:** Multiple LoRA adapters can be swapped or combined on top of a single base model (e.g., a \"style\" adapter and a \"content\" adapter for a diffusion model).\n",
      "*   **Avoids Catastrophic Forgetting:** By keeping the original weights frozen, the model retains its general knowledge.\n",
      "\n",
      "### 6. Limitations & Considerations\n",
      "\n",
      "*   **Performance:** While often achieving comparable performance to full fine-tuning, in some highly complex or divergent tasks, it might not reach the absolute peak performance of a fully fine-tuned model.\n",
      "*   **Hyperparameter Tuning:** Choosing the optimal rank ($r$) and target layers can require some experimentation.\n",
      "*   **Inference Overhead:** During inference, the $BA$ matrices need to be added to $W_0$ (either explicitly or on-the-fly), which can introduce a marginal computational overhead compared to a fully merged model.\n",
      "\n",
      "### 7. Applications\n",
      "\n",
      "*   **Large Language Models (LLMs):** Fine-tuning for instruction following, domain adaptation, summarization, specific persona generation.\n",
      "*   **Diffusion Models (e.g., Stable Diffusion):** Adapting models to generate specific styles, characters, or objects without retraining the entire model.\n",
      "*   **Other Transformer-based Models:** Any large pre-trained model with dense layers can potentially benefit from LoRA.\n",
      "\n",
      "---\n",
      "\n",
      "## Detailed Notes on QLoRA (Quantized Low-Rank Adaptation)\n",
      "\n",
      "### 1. Introduction & Core Idea\n",
      "\n",
      "QLoRA (Quantized LoRA) is an advancement over LoRA that takes parameter-efficient fine-tuning to the next level by significantly reducing memory usage, enabling the fine-tuning of *much larger* models (e.g., 65B, 70B parameters) on single consumer GPUs. It achieves this by quantizing the pre-trained base model weights to 4-bit precision while still training LoRA adapters in full (or bfloat16) precision.\n",
      "\n",
      "### 2. Motivation: Why QLoRA was Developed\n",
      "\n",
      "Even with LoRA, fine-tuning extremely large models (e.g., 65B parameters and above) can still exceed the memory capacity of typical consumer GPUs (e.g., 24GB, 48GB). This is because while LoRA reduces the *trainable* parameters, the *base model weights* still need to be loaded into memory, along with activations, gradients, and optimizer states. QLoRA aims to reduce the memory footprint of the base model itself.\n",
      "\n",
      "### 3. How QLoRA Works (Technical Details)\n",
      "\n",
      "QLoRA builds upon LoRA by introducing three key innovations:\n",
      "\n",
      "1.  **4-bit NormalFloat (NF4) Quantization:**\n",
      "    *   The most significant innovation. QLoRA quantizes the entire pre-trained base model to 4-bit precision using a novel data type called NormalFloat 4 (NF4).\n",
      "    *   NF4 is a quantile-based quantization scheme, meaning it maps the input values to the output values based on their quantiles. It is information-theoretically optimal for weights that are normally distributed (which is often the case for neural network weights).\n",
      "    *   This reduces the memory footprint of the base model weights by 8x compared to 32-bit float and 2x compared to 8-bit quantization.\n",
      "    *   **Crucially, the LoRA adapters themselves are *not* quantized.** They are trained in higher precision (e.g., bfloat16 or float32) on top of the 4-bit quantized base model. This allows the small, trainable adapters to capture fine-grained updates without performance degradation from quantization.\n",
      "    *   During the forward pass, the 4-bit quantized weights are de-quantized to bfloat16 for computation, but only the de-quantization constants are stored in higher precision, minimizing memory.\n",
      "\n",
      "2.  **Double Quantization:**\n",
      "    *   This is an additional, smaller optimization. Quantization typically involves storing the actual quantized values and also a set of \"quantization constants\" (like scales and zero-points) for each block of weights.\n",
      "    *   Double Quantization quantizes *these quantization constants themselves* to a lower precision (e.g., 8-bit float).\n",
      "    *   While the memory saving from this is relatively small (around 0.37 bits per parameter on average), it adds up across the entire model and contributes to the overall reduction.\n",
      "\n",
      "3.  **Paged Optimizers:**\n",
      "    *   Optimizer states (e.g., for AdamW, which requires two states per parameter) can consume a significant amount of GPU memory, especially for large models.\n",
      "    *   QLoRA uses NVIDIA's Unified Memory feature to automatically page optimizer states between GPU VRAM and CPU RAM.\n",
      "    *   When an optimizer state is needed for computation, it's paged onto the GPU; when not in active use, it can be paged back to the CPU. This prevents Out-Of-Memory (OOM) errors that can occur when peak memory usage for optimizer states exceeds GPU capacity.\n",
      "\n",
      "### 4. Key Features & Parameters\n",
      "\n",
      "*   **4-bit NF4 Quantization:** Core technology for base model weights.\n",
      "*   **Double Quantization:** Secondary memory saving for quantization constants.\n",
      "*   **Paged Optimizers:** Manages optimizer state memory to prevent OOM.\n",
      "*   **LoRA Adapters:** Applied on top of the quantized base model, trained in higher precision (bfloat16/float32).\n",
      "\n",
      "### 5. Benefits of QLoRA\n",
      "\n",
      "*   **Unprecedented Memory Efficiency:** Enables fine-tuning of models like Llama-2 70B on a single 48GB GPU, or Llama-2 65B on a single 24GB GPU.\n",
      "*   **Accessibility:** Makes advanced LLM fine-tuning accessible to researchers and developers with more limited hardware.\n",
      "*   **Near Full-Precision Performance:** Despite the 4-bit quantization of the base model, QLoRA often achieves performance comparable to full fine-tuning or full-precision LoRA, thanks to the high-precision LoRA adapters and the robust NF4 quantization.\n",
      "*   **All Benefits of LoRA:** Inherits the benefits of LoRA, such as smaller checkpoints, faster training (for the adapters), and prevention of catastrophic forgetting.\n",
      "\n",
      "### 6. Limitations & Considerations\n",
      "\n",
      "*   **Complexity:** The underlying implementation (e.g., `bitsandbytes` library) is more complex than plain LoRA due to the quantization and memory management.\n",
      "*   **Minor Performance Differences:** While generally close, there might be very subtle performance differences compared to full-precision LoRA or full fine-tuning on highly sensitive tasks.\n",
      "*   **Hardware Requirements:** Still requires sufficient GPU VRAM to load the quantized base model, LoRA adapters, and activations during training. Paged optimizers help, but can't eliminate all memory needs.\n",
      "*   **Inference Speed:** De-quantization on the fly during inference can introduce a small overhead, though often negligible for practical applications.\n",
      "\n",
      "### 7. Applications\n",
      "\n",
      "*   **Fine-tuning Very Large LLMs:** The primary application, allowing models like Llama-2 70B, Falcon 40B, etc., to be fine-tuned on single consumer GPUs.\n",
      "*   **Research & Development:** Accelerates experimentation with large models without requiring supercomputing resources.\n",
      "*   **Democratization of LLM Fine-tuning:** Makes advanced model customization available to a wider audience.\n",
      "\n",
      "---\n",
      "\n",
      "## Key Differences & Relationship between LoRA and QLoRA\n",
      "\n",
      "| Feature | LoRA | QLoRA |\n",
      "| :---------------- | :--------------------------------------------- | :----------------------------------------------------------------- |\n",
      "| **Core Idea** | Parameter-efficient adaptation via low-rank matrices. | Quantized LoRA: Further memory reduction via 4-bit base model quantization. |\n",
      "| **Base Model Weights** | Loaded in full precision (e.g., bfloat16/float32). | Loaded in 4-bit NormalFloat (NF4) precision. |\n",
      "| **LoRA Adapters** | Trained in full precision (or bfloat16). | Trained in full precision (or bfloat16). |\n",
      "| **Memory Usage** | Significantly reduced compared to full fine-tuning. | *Massively* reduced, enabling much larger models on consumer GPUs. |\n",
      "| **Key Innovations** | Low-rank decomposition ($BA$). | NF4 quantization, Double Quantization, Paged Optimizers. |\n",
      "| **Target Models** | Large LLMs (e.g., 7B-30B parameters). | Extremely large LLMs (e.g., 40B-70B+ parameters). |\n",
      "| **Implementation** | Simpler; often via `peft` library. | More complex; often via `bitsandbytes` + `peft` libraries. |\n",
      "| **Performance** | Generally comparable to full fine-tuning. | Near-full precision performance, often indistinguishable. |\n",
      "\n",
      "**Relationship:** QLoRA is an extension and optimization of LoRA. It *uses* the LoRA mechanism for adapter training but applies it on top of a highly quantized base model, coupled with advanced memory management techniques. LoRA made fine-tuning large models efficient; QLoRA made fine-tuning *even larger* models accessible.\n",
      "\n",
      "---\n",
      "---\n",
      "\n",
      "## Lora and QLora Quiz\n",
      "\n",
      "**Instructions:** Choose the best answer for multiple-choice questions, mark True/False statements, and provide concise explanations for the short answer questions.\n",
      "\n",
      "---\n",
      "\n",
      "### Part 1: Multiple Choice Questions\n",
      "\n",
      "1.  **What does Lora stand for?**\n",
      "    a) Low-Rank Adaptation\n",
      "    b) Large-Scale Optimization\n",
      "    c) Local Region Analysis\n",
      "    d) Learned Output Regression\n",
      "\n",
      "2.  **How does Lora primarily reduce the number of trainable parameters during fine-tuning?**\n",
      "    a) By freezing the entire base model and training a new, smaller model from scratch.\n",
      "    b) By quantizing the model weights to 4-bit before training.\n",
      "    c) By introducing small, trainable low-rank decomposition matrices alongside the frozen base model weights.\n",
      "    d) By removing layers from the original pre-trained model.\n",
      "\n",
      "3.  **Which of the following is a key advantage of using Lora for fine-tuning large language models?**\n",
      "    a) It guarantees superior performance compared to full fine-tuning in all cases.\n",
      "    b) It significantly reduces the GPU memory footprint and storage requirements for the fine-tuned model.\n",
      "    c) It eliminates the need for any pre-trained base model.\n",
      "    d) It allows for training on CPUs with comparable speed to GPUs.\n",
      "\n",
      "4.  **What is the main innovation QLora introduces compared to standard Lora?**\n",
      "    a) It uses a higher rank (r) for the adapter matrices to improve expressiveness.\n",
      "    b) It applies Lora to a base model that has been quantized to 4-bit (or other low precision).\n",
      "    c) It allows for training without any gradient accumulation.\n",
      "    d) It completely replaces the original model's weights with new, smaller ones.\n",
      "\n",
      "5.  **The primary advantage of QLora over standard Lora is:**\n",
      "    a) Improved model accuracy in all scenarios.\n",
      "    b) Reduced inference latency for the fine-tuned model.\n",
      "    c) Further significant reduction in GPU memory usage, enabling fine-tuning larger models on consumer hardware.\n",
      "    d) Elimination of the need for a pre-trained base model.\n",
      "\n",
      "6.  **In the context of Lora, what does the 'rank' parameter (`r`) primarily control?**\n",
      "    a) The number of layers to which Lora is applied.\n",
      "    b) The learning rate for the adapter weights.\n",
      "    c) The dimensionality of the low-rank matrices, impacting the expressiveness and memory usage of the adapters.\n",
      "    d) The batch size during training.\n",
      "\n",
      "7.  **Both Lora and QLora are techniques that fall under which broader category of fine-tuning methods?**\n",
      "    a) Full Fine-tuning\n",
      "    b) Parameter-Efficient Fine-Tuning (PEFT)\n",
      "    c) Transfer Learning (as a general concept, but there's a more specific answer)\n",
      "    d) Zero-Shot Learning\n",
      "\n",
      "---\n",
      "\n",
      "### Part 2: True/False Statements\n",
      "\n",
      "1.  **True / False:** Lora modifies the original weights of the pre-trained base model during training.\n",
      "2.  **True / False:** QLora typically requires more GPU memory for training than standard Lora.\n",
      "3.  **True / False:** Both Lora and QLora are designed to make fine-tuning large language models more accessible by reducing computational resources.\n",
      "4.  **True / False:** The performance of a Lora-tuned model is always superior to a fully fine-tuned model.\n",
      "\n",
      "---\n",
      "\n",
      "### Part 3: Short Answer Questions\n",
      "\n",
      "1.  **Explain the core difference in *how* Lora and QLora achieve memory savings during fine-tuning.**\n",
      "2.  **Describe a scenario where you would choose QLora over standard Lora, and explain why.**\n",
      "\n",
      "---\n",
      "\n",
      "---\n",
      "\n",
      "## Answer Key\n",
      "\n",
      "### Part 1: Multiple Choice Questions\n",
      "\n",
      "1.  **a) Low-Rank Adaptation**\n",
      "2.  **c) By introducing small, trainable low-rank decomposition matrices alongside the frozen base model weights.**\n",
      "3.  **b) It significantly reduces the GPU memory footprint and storage requirements for the fine-tuned model.**\n",
      "4.  **b) It applies Lora to a base model that has been quantized to 4-bit (or other low precision).**\n",
      "5.  **c) Further significant reduction in GPU memory usage, enabling fine-tuning larger models on consumer hardware.**\n",
      "6.  **c) The dimensionality of the low-rank matrices, impacting the expressiveness and memory usage of the adapters.**\n",
      "7.  **b) Parameter-Efficient Fine-Tuning (PEFT)**\n",
      "\n",
      "### Part 2: True/False Statements\n",
      "\n",
      "1.  **False:** Lora freezes the original base model weights and only trains the small adapter matrices.\n",
      "2.  **False:** QLora requires *less* GPU memory than standard Lora due to the quantization of the base model.\n",
      "3.  **True:** Both techniques aim to make fine-tuning more resource-efficient.\n",
      "4.  **False:** While Lora often achieves comparable performance, it's not *always* superior, and sometimes can be slightly less performant than full fine-tuning.\n",
      "\n",
      "### Part 3: Short Answer Questions\n",
      "\n",
      "1.  **Explain the core difference in *how* Lora and QLora achieve memory savings during fine-tuning.**\n",
      "    *   **Lora:** Achieves memory savings primarily by keeping the vast majority of the pre-trained base model's parameters frozen and untrainable. It only introduces and trains a small set of *new, low-rank adapter matrices*. The memory saving comes from not having to compute gradients or store optimizer states for the entire base model.\n",
      "    *   **QLora:** Achieves *additional* significant memory savings by quantizing the *entire frozen base model* to a lower precision (e.g., 4-bit) before applying Lora. This drastically reduces the memory footprint of the base model itself, making it possible to load and fine-tune much larger models on GPUs with limited VRAM. The Lora adapters themselves are still trained in a higher precision (e.g., 16-bit).\n",
      "\n",
      "2.  **Describe a scenario where you would choose QLora over standard Lora, and explain why.**\n",
      "    *   **Scenario:** You want to fine-tune a very large language model (e.g., a 70B parameter model) on a consumer-grade GPU (e.g., an NVIDIA RTX 3090 with 24GB VRAM) or a small cloud instance with limited memory.\n",
      "    *   **Why:** Standard Lora, while memory-efficient, might still require too much VRAM to load and process the full 16-bit (or 32-bit) base model, even if only the adapters are being trained. QLora's quantization of the base model to 4-bit significantly reduces its memory footprint, making it feasible to fit these massive models into more constrained GPU memory environments. This allows for training models that would otherwise be inaccessible without very expensive enterprise-grade hardware.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# this is not a good emaple but i am doing\n",
    "model = GoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template=\"Genarate the detailed Notes on the following {topic}\",\n",
    "    input_variables='topic'\n",
    "\n",
    ")\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template=\"Genarat The quiz on the follwoing {topic}\",\n",
    "    input_variables='topic',\n",
    ")\n",
    "\n",
    "prompt3 = PromptTemplate(\n",
    "    template = \"Merge the both notes and quiz into the single document.\\n notes --> {notes} and quiz --> {quiz}\",\n",
    "    input_variables=['notes', 'quiz']\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "parallel_chain = RunnableParallel(\n",
    "    {\n",
    "        'notes' : prompt1 | model | parser,\n",
    "        'quiz' : prompt2 | model | parser\n",
    "    }\n",
    ")\n",
    "\n",
    "merge_chain = prompt3 | model | parser\n",
    "\n",
    "chain = parallel_chain | merge_chain\n",
    "\n",
    "result = chain.invoke('Lora and QLora')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee68176",
   "metadata": {},
   "source": [
    "## Branch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9268ffee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                +---------------------------+                  \n",
      "                | Parallel<notes,quiz>Input |                  \n",
      "                +---------------------------+                  \n",
      "                    ***                 ***                    \n",
      "                ****                       ****                \n",
      "              **                               **              \n",
      "  +----------------+                      +----------------+   \n",
      "  | PromptTemplate |                      | PromptTemplate |   \n",
      "  +----------------+                      +----------------+   \n",
      "           *                                       *           \n",
      "           *                                       *           \n",
      "           *                                       *           \n",
      "+--------------------+                  +--------------------+ \n",
      "| GoogleGenerativeAI |                  | GoogleGenerativeAI | \n",
      "+--------------------+                  +--------------------+ \n",
      "           *                                       *           \n",
      "           *                                       *           \n",
      "           *                                       *           \n",
      "  +-----------------+                     +-----------------+  \n",
      "  | StrOutputParser |                     | StrOutputParser |  \n",
      "  +-----------------+                     +-----------------+  \n",
      "                    ***                 ***                    \n",
      "                       ****         ****                       \n",
      "                           **     **                           \n",
      "                +----------------------------+                 \n",
      "                | Parallel<notes,quiz>Output |                 \n",
      "                +----------------------------+                 \n",
      "                               *                               \n",
      "                               *                               \n",
      "                               *                               \n",
      "                      +----------------+                       \n",
      "                      | PromptTemplate |                       \n",
      "                      +----------------+                       \n",
      "                               *                               \n",
      "                               *                               \n",
      "                               *                               \n",
      "                    +--------------------+                     \n",
      "                    | GoogleGenerativeAI |                     \n",
      "                    +--------------------+                     \n",
      "                               *                               \n",
      "                               *                               \n",
      "                               *                               \n",
      "                      +-----------------+                      \n",
      "                      | StrOutputParser |                      \n",
      "                      +-----------------+                      \n",
      "                               *                               \n",
      "                               *                               \n",
      "                               *                               \n",
      "                  +-----------------------+                    \n",
      "                  | StrOutputParserOutput |                    \n",
      "                  +-----------------------+                    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "graph = chain.get_graph()\n",
    "graph.print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032bd5dc",
   "metadata": {},
   "source": [
    "## Conditional Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b551f35c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9217ce58",
   "metadata": {},
   "source": [
    "- Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541bb6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758805298.822171   40982 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch,RunnableLambda\n",
    "from pydantic import BaseModel,Field\n",
    "from typing import Literal\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model = 'gemini-2.5-flash')\n",
    "\n",
    "class Feedback(BaseModel):\n",
    "    sentiment : Literal['positive','negative'] = Field(description='Give the sentiment of the review')\n",
    "\n",
    "parser2 = PydanticOutputParser(\n",
    "    pydantic_object=Feedback\n",
    ")\n",
    "\n",
    "prompt1= PromptTemplate(\n",
    "    template=\"Give the sentiment of the review-->{review} \",\n",
    "    input_variables=['review']\n",
    "   \n",
    ")\n",
    "\n",
    "\n",
    "review = \"\"\"So I've had the unit for a little under a month now, here's what I reckon\n",
    "\n",
    "Taking everything about these glasses individually, nothing is great. Camera - not great. Headphones - not great. Battery - not great. Price - not great. Functioning as a pair of glasses on their own as well, not great. Pretty open and shut, just another average product then right?\n",
    "\n",
    "Well, I would say that for everything that isn't great, every single thing about these glasses is actually \"good enough\". As you read further you'll probably agree that's a pretty damn tough bar to clear in a pair of sunglasses that, in my view, look 95% comparable to a regular pair of sunglasses.\n",
    "\n",
    "The reason I bought these - my 2 year old stops whatever he's doing when I pull out my phone. He comes over to me and says \"take photo\" then steals my phone to look at the photo we've taken. It's cute, but it makes for impossible candid videos. After some deliberation I forked out the $550 AUD on these, and with a 2nd child on the way, the price seemed to halve in my head. I love listening to music and I love gadgets, but these are moreso the reason I knew these glasses existed as opposed to the reason I wanted them.\n",
    "\n",
    "For my below use cases, these glasses are absolutely exceptional:\n",
    "\n",
    "taking candid short videos on the fly\n",
    "\n",
    "walking the dog + listening to music. (Potentially also for running, but I haven't tried this yet.)\n",
    "\n",
    "blocking out the sun while doing tasks 1 and 2.\n",
    "\n",
    "Videos on these glasses are simply exceptional. Quality-wise, they more than suffice. Just have a look on this reddit page and you can make your own mind up. I've switched the single button press from taking a photo to starting a video. When you have a 2 year old who for no reason starts singing NSYNC's classic Bye Bye Bye out of nowhere, every second matters if you go reaching for your phone. Press the power button, camera button, video mode, press record, hold phone up to subject. I usually bail after step 2 because I've already taken myself out of the moment and want to get back in - I typically hate taking videos for that reason. Different for everyone, but for me this product really hits the spot here. I love starting videos instantly and jumping right back into the moment to experience every bit of it. I love that I have 2 hands to pick my son up and spin him around to tell him how he sings better than any member of NSYNC ever did - the video will stop recording automatically after 3 mins anyway so just forget it's even happening. The moments I have captured from these glasses make me emotional to think that without these glasses, in 20 years those memories would have been relegated to a folder in my brain labeled \"good old days - 2024\". For me, this is more than enough reason to own this product.\n",
    "\n",
    "My next use case - listening to music while going walking. No headphones to take, keeps you alert with outside noise still coming in at full volume, and they're comfortable so you forget you're wearing them. I would not recommend taking them in built up areas, beacuse the sound quality above 75% is borderline unlistenable, and also not at all loud. Within mid-low noise environments (the suburbs for eg), 40-50% volume works great. They are my favourite way to experience my walks at the moment, however they are not $550 better than decent pair of headphones and a pair of sunnies IMO. Going for walks is a good complimentary reason to own these in my view.\n",
    "\n",
    "As for lenses, I picked the transitions so that I could record videos indoors. They were around $80 AUD more expensive, but definitely worth it - probably 75%% of my videos are indoors.\n",
    "\n",
    "Here are some use cases I thought would be cool but turned out meh:\n",
    "\n",
    "taking calls (I don't wear these as much as I thought so it rarely happens...but when I do get a call it's a nice little treat, very easy to use)\n",
    "\n",
    "living and breathing music any time any place - same with point 1, I don't wear these as often as I thought I would.\n",
    "\n",
    "taking photos - just use your phone. If there's anything you need to take urgently, just start a video on your glasses instead. For photos, you can frame things better with a phone and the quality won't suffer. The quality of photos on these glasses is pretty grim, decent colours and contrast, but really poor detail.\n",
    "\n",
    "Some more tid bits:\n",
    "\n",
    "The battery is terrible but only if you use them constantly, turn off \"hey meta\" to improve it and overall use them sparingly to get a day's worth of use. Music is fine, but photos and videos are the real battery killers. The case is awesome so bring it on long days. The look of the glasses is great. Nobody realises they have cameras. I don't feel self conscious wearing them. They hurt when I started wearing them initially, but I lightly warmed up the middle ridge and bent them out just a fraction, plus I've worn them in a bit, they feel fine now. I often wonder how much battery is left on them, so I turn them off and on again because it tells you on startup. I accidentally take videos when I take them off sometimes because I press the button unintentionally. You need to triple tap the side of the glasses to go back a song, and and sometimes registration of taps is garbage. God help you if you want to go back 2 songs, nailing 2 triple taps in a row is no easy feat. I am sometimes scared I'll break them because they're light and plastic. I got a bit sick at the start wearing them for hours at a time because the lenses distort your vision just a tiny fraction, but that feels fine now. My son notices the LED sometimes when i'm recording, but he's cool with it most of the time.\n",
    "\n",
    "To close - it's clear that Meta + Raybans have done their homework with this product, and I love that. Every detail from the size and placement of the action button on the frame, to the clickyness of said button, to the placement of an internal notification LED in the perfect spot, to the quality of the video being obviously prioritised over the photos (good call Meta), to making an outward facing LED that looks just like the camera for aesthetic consistency, to the way the glasses click down into the charging case, to the syncing of photos/videos occurring automatically when the glasses are charging at home and hooked up to wifi. These are some of the little details you may not notice when you pick these up, and none of them are deal breakers or major selling points, but put them all together and you have an experience with this product that is well researched, smooth, ultimately just bloody good.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "chain = prompt1 | model | parser\n",
    "result = chain.invoke({'review':review})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faf6f982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of this review is **overall positive, despite acknowledging numerous flaws and limitations.**\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "**Why it's Positive:**\n",
      "\n",
      "*   **Core Use Case Excels:** The reviewer explicitly states that for their primary reason for purchase (taking candid short videos of their child), the glasses are \"absolutely exceptional\" and \"simply exceptional,\" truly \"hitting the spot.\"\n",
      "*   **Emotional Connection:** The ability to capture fleeting moments with their child makes the reviewer \"emotional\" and provides \"more than enough reason to own this product.\" This is a very strong positive.\n",
      "*   **\"Good Enough\" is a Win:** Despite nothing being \"great\" individually, the reviewer finds everything to be \"good enough,\" which is a \"pretty damn tough bar to clear.\"\n",
      "*   **Secondary Use Case Positive:** Listening to music while walking is described as their \"favourite way to experience my walks at the moment.\"\n",
      "*   **Design & Experience Appreciation:** The closing paragraph praises Meta and Ray-Ban for \"doing their homework,\" loving the details, and concluding the experience is \"well researched, smooth, ultimately just bloody good.\"\n",
      "*   **Aesthetics & Comfort:** The glasses \"look great,\" \"nobody realises they have cameras,\" and they don't feel self-conscious. Initial discomfort was resolved.\n",
      "\n",
      "**Why it's Not Purely Positive (Acknowledged Negatives/Limitations):**\n",
      "\n",
      "*   **Individual Component Weaknesses:** \"Camera - not great. Headphones - not great. Battery - not great. Price - not great.\"\n",
      "*   **\"Meh\" Use Cases:** Taking calls, constant music listening, and especially taking photos are deemed inferior to using a phone. Photo quality is \"pretty grim.\"\n",
      "*   **Battery Life:** Described as \"terrible\" if used constantly.\n",
      "*   **User Interface Issues:** Accidental videos, \"garbage\" tap registration for skipping songs.\n",
      "*   **Fragility Concerns:** \"Scared I'll break them because they're light and plastic.\"\n",
      "*   **Sound Quality:** Music above 75% volume is \"borderline unlistenable\" and \"not at all loud.\"\n",
      "*   **Value for Music:** \"Not $550 better than decent pair of headphones and a pair of sunnies.\"\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The reviewer clearly loves the product for its unique ability to fulfill their specific, high-priority use cases, particularly candid video capture, and appreciates the overall design and user experience. While they are very honest about its many shortcomings and areas where it doesn't excel, these negatives are ultimately outweighed by the profound value and convenience it provides for their primary needs.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7bc45",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21348304",
   "metadata": {},
   "source": [
    "**Note** : if we obsereve if we directly ask the model it dotn  provide\n",
    "exact answer what we need as we need only positive or negative but it given so much of the text also ...so what we do is that parser the output of the \n",
    "model insted "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba19fab5",
   "metadata": {},
   "source": [
    "- Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07c56954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758806375.879103   40982 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch,RunnableLambda\n",
    "from pydantic import BaseModel,Field\n",
    "from typing import Literal\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model = 'gemini-2.5-flash')\n",
    "\n",
    "class Feedback(BaseModel):\n",
    "    sentiment : Literal['positive','negative'] = Field(description='Give the sentiment of the review')\n",
    "\n",
    "parser2 = PydanticOutputParser(\n",
    "    pydantic_object=Feedback\n",
    ")\n",
    "prompt1= PromptTemplate(\n",
    "    template=\"Give the sentiment of the review-->{review} \\n {format_instruction}\",\n",
    "    input_variables=['review'],\n",
    "    partial_variables={'format_instruction':parser2.get_format_instructions}\n",
    ")\n",
    "\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template=\"Provide response for the positive feedback.\\n {review}\",\n",
    "    input_variables='review'\n",
    ")\n",
    "\n",
    "prompt3 = PromptTemplate(\n",
    "    template=\"Provide response for the negative feedback.\\n {review}\",\n",
    "    input_variables='review'\n",
    ")\n",
    "\n",
    "review = \"\"\"So I've had the unit for a little under a month now, here's what I reckon\n",
    "\n",
    "Taking everything about these glasses individually, nothing is great. Camera - not great. Headphones - not great. Battery - not great. Price - not great. Functioning as a pair of glasses on their own as well, not great. Pretty open and shut, just another average product then right?\n",
    "\n",
    "Well, I would say that for everything that isn't great, every single thing about these glasses is actually \"good enough\". As you read further you'll probably agree that's a pretty damn tough bar to clear in a pair of sunglasses that, in my view, look 95% comparable to a regular pair of sunglasses.\n",
    "\n",
    "The reason I bought these - my 2 year old stops whatever he's doing when I pull out my phone. He comes over to me and says \"take photo\" then steals my phone to look at the photo we've taken. It's cute, but it makes for impossible candid videos. After some deliberation I forked out the $550 AUD on these, and with a 2nd child on the way, the price seemed to halve in my head. I love listening to music and I love gadgets, but these are moreso the reason I knew these glasses existed as opposed to the reason I wanted them.\n",
    "\n",
    "For my below use cases, these glasses are absolutely exceptional:\n",
    "\n",
    "taking candid short videos on the fly\n",
    "\n",
    "walking the dog + listening to music. (Potentially also for running, but I haven't tried this yet.)\n",
    "\n",
    "blocking out the sun while doing tasks 1 and 2.\n",
    "\n",
    "Videos on these glasses are simply exceptional. Quality-wise, they more than suffice. Just have a look on this reddit page and you can make your own mind up. I've switched the single button press from taking a photo to starting a video. When you have a 2 year old who for no reason starts singing NSYNC's classic Bye Bye Bye out of nowhere, every second matters if you go reaching for your phone. Press the power button, camera button, video mode, press record, hold phone up to subject. I usually bail after step 2 because I've already taken myself out of the moment and want to get back in - I typically hate taking videos for that reason. Different for everyone, but for me this product really hits the spot here. I love starting videos instantly and jumping right back into the moment to experience every bit of it. I love that I have 2 hands to pick my son up and spin him around to tell him how he sings better than any member of NSYNC ever did - the video will stop recording automatically after 3 mins anyway so just forget it's even happening. The moments I have captured from these glasses make me emotional to think that without these glasses, in 20 years those memories would have been relegated to a folder in my brain labeled \"good old days - 2024\". For me, this is more than enough reason to own this product.\n",
    "\n",
    "My next use case - listening to music while going walking. No headphones to take, keeps you alert with outside noise still coming in at full volume, and they're comfortable so you forget you're wearing them. I would not recommend taking them in built up areas, beacuse the sound quality above 75% is borderline unlistenable, and also not at all loud. Within mid-low noise environments (the suburbs for eg), 40-50% volume works great. They are my favourite way to experience my walks at the moment, however they are not $550 better than decent pair of headphones and a pair of sunnies IMO. Going for walks is a good complimentary reason to own these in my view.\n",
    "\n",
    "As for lenses, I picked the transitions so that I could record videos indoors. They were around $80 AUD more expensive, but definitely worth it - probably 75%% of my videos are indoors.\n",
    "\n",
    "Here are some use cases I thought would be cool but turned out meh:\n",
    "\n",
    "taking calls (I don't wear these as much as I thought so it rarely happens...but when I do get a call it's a nice little treat, very easy to use)\n",
    "\n",
    "living and breathing music any time any place - same with point 1, I don't wear these as often as I thought I would.\n",
    "\n",
    "taking photos - just use your phone. If there's anything you need to take urgently, just start a video on your glasses instead. For photos, you can frame things better with a phone and the quality won't suffer. The quality of photos on these glasses is pretty grim, decent colours and contrast, but really poor detail.\n",
    "\n",
    "Some more tid bits:\n",
    "\n",
    "The battery is terrible but only if you use them constantly, turn off \"hey meta\" to improve it and overall use them sparingly to get a day's worth of use. Music is fine, but photos and videos are the real battery killers. The case is awesome so bring it on long days. The look of the glasses is great. Nobody realises they have cameras. I don't feel self conscious wearing them. They hurt when I started wearing them initially, but I lightly warmed up the middle ridge and bent them out just a fraction, plus I've worn them in a bit, they feel fine now. I often wonder how much battery is left on them, so I turn them off and on again because it tells you on startup. I accidentally take videos when I take them off sometimes because I press the button unintentionally. You need to triple tap the side of the glasses to go back a song, and and sometimes registration of taps is garbage. God help you if you want to go back 2 songs, nailing 2 triple taps in a row is no easy feat. I am sometimes scared I'll break them because they're light and plastic. I got a bit sick at the start wearing them for hours at a time because the lenses distort your vision just a tiny fraction, but that feels fine now. My son notices the LED sometimes when i'm recording, but he's cool with it most of the time.\n",
    "\n",
    "To close - it's clear that Meta + Raybans have done their homework with this product, and I love that. Every detail from the size and placement of the action button on the frame, to the clickyness of said button, to the placement of an internal notification LED in the perfect spot, to the quality of the video being obviously prioritised over the photos (good call Meta), to making an outward facing LED that looks just like the camera for aesthetic consistency, to the way the glasses click down into the charging case, to the syncing of photos/videos occurring automatically when the glasses are charging at home and hooked up to wifi. These are some of the little details you may not notice when you pick these up, and none of them are deal breakers or major selling points, but put them all together and you have an experience with this product that is well researched, smooth, ultimately just bloody good.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "chain = prompt1 | model | parser2\n",
    "result = chain.invoke({'review':review})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ae614ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "print(result.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e190d0",
   "metadata": {},
   "source": [
    "- step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02d0e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"\"\"So I've had the unit for a little under a month now, here's what I reckon\n",
    "\n",
    "Taking everything about these glasses individually, nothing is great. Camera - not great. Headphones - not great. Battery - not great. Price - not great. Functioning as a pair of glasses on their own as well, not great. Pretty open and shut, just another average product then right?\n",
    "\n",
    "Well, I would say that for everything that isn't great, every single thing about these glasses is actually \"good enough\". As you read further you'll probably agree that's a pretty damn tough bar to clear in a pair of sunglasses that, in my view, look 95% comparable to a regular pair of sunglasses.\n",
    "\n",
    "The reason I bought these - my 2 year old stops whatever he's doing when I pull out my phone. He comes over to me and says \"take photo\" then steals my phone to look at the photo we've taken. It's cute, but it makes for impossible candid videos. After some deliberation I forked out the $550 AUD on these, and with a 2nd child on the way, the price seemed to halve in my head. I love listening to music and I love gadgets, but these are moreso the reason I knew these glasses existed as opposed to the reason I wanted them.\n",
    "\n",
    "For my below use cases, these glasses are absolutely exceptional:\n",
    "\n",
    "taking candid short videos on the fly\n",
    "\n",
    "walking the dog + listening to music. (Potentially also for running, but I haven't tried this yet.)\n",
    "\n",
    "blocking out the sun while doing tasks 1 and 2.\n",
    "\n",
    "Videos on these glasses are simply exceptional. Quality-wise, they more than suffice. Just have a look on this reddit page and you can make your own mind up. I've switched the single button press from taking a photo to starting a video. When you have a 2 year old who for no reason starts singing NSYNC's classic Bye Bye Bye out of nowhere, every second matters if you go reaching for your phone. Press the power button, camera button, video mode, press record, hold phone up to subject. I usually bail after step 2 because I've already taken myself out of the moment and want to get back in - I typically hate taking videos for that reason. Different for everyone, but for me this product really hits the spot here. I love starting videos instantly and jumping right back into the moment to experience every bit of it. I love that I have 2 hands to pick my son up and spin him around to tell him how he sings better than any member of NSYNC ever did - the video will stop recording automatically after 3 mins anyway so just forget it's even happening. The moments I have captured from these glasses make me emotional to think that without these glasses, in 20 years those memories would have been relegated to a folder in my brain labeled \"good old days - 2024\". For me, this is more than enough reason to own this product.\n",
    "\n",
    "My next use case - listening to music while going walking. No headphones to take, keeps you alert with outside noise still coming in at full volume, and they're comfortable so you forget you're wearing them. I would not recommend taking them in built up areas, beacuse the sound quality above 75% is borderline unlistenable, and also not at all loud. Within mid-low noise environments (the suburbs for eg), 40-50% volume works great. They are my favourite way to experience my walks at the moment, however they are not $550 better than decent pair of headphones and a pair of sunnies IMO. Going for walks is a good complimentary reason to own these in my view.\n",
    "\n",
    "As for lenses, I picked the transitions so that I could record videos indoors. They were around $80 AUD more expensive, but definitely worth it - probably 75%% of my videos are indoors.\n",
    "\n",
    "Here are some use cases I thought would be cool but turned out meh:\n",
    "\n",
    "taking calls (I don't wear these as much as I thought so it rarely happens...but when I do get a call it's a nice little treat, very easy to use)\n",
    "\n",
    "living and breathing music any time any place - same with point 1, I don't wear these as often as I thought I would.\n",
    "\n",
    "taking photos - just use your phone. If there's anything you need to take urgently, just start a video on your glasses instead. For photos, you can frame things better with a phone and the quality won't suffer. The quality of photos on these glasses is pretty grim, decent colours and contrast, but really poor detail.\n",
    "\n",
    "Some more tid bits:\n",
    "\n",
    "The battery is terrible but only if you use them constantly, turn off \"hey meta\" to improve it and overall use them sparingly to get a day's worth of use. Music is fine, but photos and videos are the real battery killers. The case is awesome so bring it on long days. The look of the glasses is great. Nobody realises they have cameras. I don't feel self conscious wearing them. They hurt when I started wearing them initially, but I lightly warmed up the middle ridge and bent them out just a fraction, plus I've worn them in a bit, they feel fine now. I often wonder how much battery is left on them, so I turn them off and on again because it tells you on startup. I accidentally take videos when I take them off sometimes because I press the button unintentionally. You need to triple tap the side of the glasses to go back a song, and and sometimes registration of taps is garbage. God help you if you want to go back 2 songs, nailing 2 triple taps in a row is no easy feat. I am sometimes scared I'll break them because they're light and plastic. I got a bit sick at the start wearing them for hours at a time because the lenses distort your vision just a tiny fraction, but that feels fine now. My son notices the LED sometimes when i'm recording, but he's cool with it most of the time.\n",
    "\n",
    "To close - it's clear that Meta + Raybans have done their homework with this product, and I love that. Every detail from the size and placement of the action button on the frame, to the clickyness of said button, to the placement of an internal notification LED in the perfect spot, to the quality of the video being obviously prioritised over the photos (good call Meta), to making an outward facing LED that looks just like the camera for aesthetic consistency, to the way the glasses click down into the charging case, to the syncing of photos/videos occurring automatically when the glasses are charging at home and hooked up to wifi. These are some of the little details you may not notice when you pick these up, and none of them are deal breakers or major selling points, but put them all together and you have an experience with this product that is well researched, smooth, ultimately just bloody good.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3be04757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758806906.708025   40982 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's wonderful to hear! Here are a few options, choose the one that best fits your company's tone:\n",
      "\n",
      "**Option 1 (Concise & Enthusiastic):**\n",
      "\n",
      "> Thank you so much for your positive feedback! We're absolutely delighted to hear you had such a great experience. Your kind words truly made our day! We look forward to serving you again soon.\n",
      "\n",
      "**Option 2 (Slightly More Detailed):**\n",
      "\n",
      "> We truly appreciate you taking the time to share your positive feedback! It's incredibly rewarding to know that you had such a wonderful experience with us. Your support means a great deal to our team, and we're committed to continuing to provide excellent service. Thank you again!\n",
      "\n",
      "**Option 3 (Warm & Personal):**\n",
      "\n",
      "> Wow, thank you so much for your incredibly kind words! We were absolutely thrilled to read your positive feedback. Knowing that we could provide you with a great experience is the best reward for our team. We're so grateful for your support and hope to see you again soon!\n",
      "\n",
      "---\n",
      "\n",
      "**Key elements used:**\n",
      "\n",
      "*   **Immediate thanks:** \"Thank you so much,\" \"We truly appreciate.\"\n",
      "*   **Expression of delight/reward:** \"absolutely delighted,\" \"wonderful experience,\" \"incredibly rewarding,\" \"thrilled.\"\n",
      "*   **Acknowledgement of effort:** Implies the team's hard work.\n",
      "*   **Value of feedback:** \"Your kind words truly made our day,\" \"Your support means a great deal.\"\n",
      "*   **Forward-looking:** \"look forward to serving you again,\" \"hope to see you again soon.\"\n",
      "*   **Warm closing:** (Implied, but you'd add your name/company name)\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch, RunnableLambda\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model='gemini-2.5-flash')\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Step 1: Define schema for sentiment\n",
    "class Feedback(BaseModel):\n",
    "    sentiment: Literal['positive', 'negative'] = Field(\n",
    "        description=\"Classify the review sentiment as either 'positive' or 'negative'.\"\n",
    "    )\n",
    "\n",
    "parser2 = PydanticOutputParser(pydantic_object=Feedback)\n",
    "\n",
    "# Step 2: Prompt for classification\n",
    "prompt1 = PromptTemplate(\n",
    "    template=\"\"\"You are a strict sentiment classifier.\n",
    "Review: \"{review}\"\n",
    "\n",
    "Classify the above review as either:\n",
    "- positive\n",
    "- negative\n",
    "\n",
    "Return the output in the required format:\n",
    "{format_instruction}\n",
    "\"\"\",\n",
    "    input_variables=['review'],\n",
    "    partial_variables={'format_instruction': parser2.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Step 3: Prompts for responses\n",
    "prompt2 = PromptTemplate(\n",
    "    template=\"The customer gave positive feedback. Write a warm and appreciative reply:\\n\\n{review}\",\n",
    "    input_variables=['review'],\n",
    ")\n",
    "\n",
    "prompt3 = PromptTemplate(\n",
    "    template=\"The customer gave negative feedback. Write a polite and empathetic reply with an offer to improve:\\n\\n{review}\",\n",
    "    input_variables=['review'],\n",
    ")\n",
    "\n",
    "# Step 4: Build classifier chain\n",
    "classifier_chain = prompt1 | model | parser2\n",
    "\n",
    "# Step 5: Conditional chain based on classification\n",
    "conditional_chain = RunnableBranch(\n",
    "    (lambda x: x.sentiment == 'positive', prompt2 | model | parser),\n",
    "    (lambda x: x.sentiment == 'negative', prompt3 | model | parser),\n",
    "    RunnableLambda(lambda x: \"The sentiment is neutral.\")\n",
    ")\n",
    "\n",
    "# Step 6: Full chain\n",
    "chain = classifier_chain | conditional_chain\n",
    "\n",
    "# Example run\n",
    "result = chain.invoke({'review': 'This mobile  was fantastic'})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e7c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2778408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When responding to negative feedback, the goal is to acknowledge the customer's feelings, apologize for the negative experience, take responsibility, and offer a path to resolution or improvement.\n",
      "\n",
      "Here are several appropriate responses, depending on the specific nature of the negative feedback.\n",
      "\n",
      "---\n",
      "\n",
      "**Key Principles for Responding to Negative Feedback:**\n",
      "\n",
      "1.  **Acknowledge and Thank:** Thank them for taking the time to provide feedback.\n",
      "2.  **Empathize and Apologize:** Show you understand their frustration and sincerely apologize for the negative experience.\n",
      "3.  **Take Responsibility:** Avoid blame. Focus on what *you* or *your organization* can do.\n",
      "4.  **Offer a Solution or Path to Resolution:** What concrete steps can be taken?\n",
      "5.  **Ask for More Information (if needed):** If the feedback is vague, ask for details to understand and resolve the issue.\n",
      "6.  **Maintain a Professional Tone:** Stay calm and respectful.\n",
      "7.  **Commit to Improvement:** Reassure them that their feedback is valuable and will be used to improve.\n",
      "\n",
      "---\n",
      "\n",
      "**General Template:**\n",
      "\n",
      "\"Thank you for sharing your feedback. I'm truly sorry to hear that [specific aspect of their negative experience, or \"your experience\"] was not positive/did not meet your expectations. We take all feedback seriously and would like to [offer solution / ask for more details / explain next steps]. We are committed to improving and appreciate you bringing this to our attention.\"\n",
      "\n",
      "---\n",
      "\n",
      "**Specific Examples:**\n",
      "\n",
      "**1. For General Dissatisfaction / Vague Negative Feedback:**\n",
      "\n",
      "> \"Thank you for taking the time to share your feedback. I'm truly sorry to hear that your experience with us was not positive. We are always striving to improve, and your input is valuable. Could you please provide more details about what specifically led to your dissatisfaction so we can better understand and address the issue? Please feel free to reply directly to this message or contact our support team at [phone number/email].\"\n",
      "\n",
      "**2. For a Specific Product/Service Issue (e.g., a bug, a broken item):**\n",
      "\n",
      "> \"Thank you for bringing this to our attention. I sincerely apologize that you've experienced issues with [mention specific product/service, e.g., 'the software crashing' or 'your recent order']. That sounds incredibly frustrating, and it's certainly not the quality we aim to provide.\n",
      ">\n",
      "> We'd like to investigate this immediately and help resolve it for you. Could you please provide your [order number/account ID/specific details about the issue]? Please contact our support team directly at [phone number/email] or reply to this message, and we will work to find a solution as quickly as possible.\"\n",
      "\n",
      "**3. For a Poor Customer Service Experience:**\n",
      "\n",
      "> \"Thank you for sharing your experience. I am very sorry to hear that your recent interaction with our customer service team was unsatisfactory. That is certainly not the standard of service we strive for, and I apologize for any frustration or inconvenience this caused.\n",
      ">\n",
      "> We take feedback like this very seriously and will be reviewing this internally to understand what went wrong and prevent it from happening again. Could you please provide more details about your interaction (e.g., date, time, agent name if possible) so we can look into this further? We are dedicated to providing excellent support and are disappointed we fell short.\"\n",
      "\n",
      "**4. For a Missing Feature or Product Limitation:**\n",
      "\n",
      "> \"Thank you for your feedback regarding [specific feature/aspect]. I understand your frustration that [the feature you mentioned] is not currently available or didn't meet your expectations in this area.\n",
      ">\n",
      "> We're always looking for ways to improve our [product/service], and your input is incredibly valuable in guiding our development. I've noted your suggestion, and we'll certainly take it into consideration for future updates. While I can't promise an immediate change, we appreciate you helping us make our [product/service] better.\"\n",
      "\n",
      "**5. For Delays or Unmet Expectations:**\n",
      "\n",
      "> \"Thank you for reaching out. I sincerely apologize for the delay you've experienced with [mention specific delay, e.g., 'your order's delivery' or 'our response time'] and for any inconvenience or frustration this has caused. We understand how important [the product/service] is to you, and we regret not meeting your expectations in this instance.\n",
      ">\n",
      "> We are actively working to [explain what you're doing to fix it, e.g., 'resolve the backlog' or 'track down your shipment']. We will keep you updated on the progress, and please feel free to contact us at [phone number/email] if you have any further questions.\"\n",
      "\n",
      "---\n",
      "\n",
      "Choose the response that best fits the situation, and remember to personalize it where possible!\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f833394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      +-------------+      \n",
      "      | PromptInput |      \n",
      "      +-------------+      \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+------------------------+ \n",
      "| ChatGoogleGenerativeAI | \n",
      "+------------------------+ \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      " +----------------------+  \n",
      " | PydanticOutputParser |  \n",
      " +----------------------+  \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "        +--------+         \n",
      "        | Branch |         \n",
      "        +--------+         \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "     +--------------+      \n",
      "     | BranchOutput |      \n",
      "     +--------------+      \n"
     ]
    }
   ],
   "source": [
    "\n",
    "graph = chain.get_graph()\n",
    "graph.print_ascii()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
