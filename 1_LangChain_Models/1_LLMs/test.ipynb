{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b332b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed1f2ad7",
   "metadata": {},
   "source": [
    "This is the Transformers with distel bart which is\n",
    "encoder and decoder architecture.\n",
    "There is other tools also avilable like T5\n",
    "These are seqtoseq models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b3471b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/rgukt/data/LangChain/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hareesh was a good boy who always tried to do the right thing . He approached a girl who was very beautiful and asked her name, but she didn't reply . Hareesh felt a little disappointed and confused, but he didn't let it bother him too much . A few days later, Hareesh remembered and called her by her name .\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, use_safetensors=True)\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=0)\n",
    "text = \"\"\"Hareesh was a good boy who always tried to do the right thing. One day, when he went to school, he noticed a girl who was very beautiful. Curious, he approached her and asked her name, but she did not reply. Hareesh felt a little disappointed and confused, but he didn’t let it bother him too much.\n",
    "\n",
    "A few days later, the girl came near Hareesh and asked him, “What is your name?” This time, Hareesh answered confidently, “I am Hareesh.” However, he forgot to ask her name in return. The girl felt a little disappointed that he hadn’t shown interest in her name.\n",
    "\n",
    "Two days later, Hareesh remembered and called her by her name. She was overjoyed that he had taken the effort to remember her. From that moment, they started talking more and became good friends.\n",
    "\n",
    "As their friendship grew, they decided to spend a day together at the beach. They played in the sand, collected seashells, and enjoyed the sound of the waves crashing on the shore. They shared stories, laughed at each other’s jokes, and even watched the sunset together. That day at the beach became a cherished memory for both of them, marking the beginning of a beautiful friendship built on care, attention, and mutual respect.\n",
    "\"\"\"\n",
    "result = summarizer(text, max_length=130, min_length=30, do_sample=False)\n",
    "print(result[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4e7bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7a54949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "Device name: CPU only\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU only\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6bc2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2e6199e",
   "metadata": {},
   "source": [
    "# 1. LLMs (Large Language Models)\n",
    "These are the most basic models in LangChain. They follow a simple text-in, text-out interface. You provide a string of text (a prompt), and the model returns a string of text as a completion.\n",
    "\n",
    "Code Example\n",
    "This example uses Google's flan-t5-xxl, a great instruction-following model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c27414c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: who is the current president of the India\n",
      "Answer: Jawahar Lal Nehru\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "is_gpu_available = torch.cuda.is_available()\n",
    "device = 0 if is_gpu_available else -1 # Use GPU if available, otherwise CPU\n",
    "print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n",
    "\n",
    "# 1. Use an instruction-tuned model designed for Q&A\n",
    "model_name = \"google/flan-t5-large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# 2. Use the \"text2text-generation\" pipeline for this type of model\n",
    "question_answerer = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# 3. Ask your question\n",
    "question = \"who is the current president of the India\"\n",
    "result = question_answerer(question)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", result[0]['generated_text'])\n",
    "\n",
    "# Example Output:\n",
    "# Question: What is the capital of India?\n",
    "# Answer: New Delhi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
