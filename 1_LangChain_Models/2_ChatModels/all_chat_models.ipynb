{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "600db0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c7046",
   "metadata": {},
   "source": [
    "# Chat Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea123fd",
   "metadata": {},
   "source": [
    "## loading the chat model from the hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52855a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is currently the year 2023. I hope this helps!\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",  # Hosted on API\n",
    "    task=\"text-generation\"\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "result = model.invoke(\"what is the current year\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d8dee",
   "metadata": {},
   "source": [
    "## Api call using google_geni "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935bcd41",
   "metadata": {},
   "source": [
    "We can make repective anthorpic and open ai class if neccessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b328bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API keys loaded. Initializing Google Gemini model...\n",
      "ðŸš€ Model ready. Asking question...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758618694.069631   53596 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Response ---\n",
      "The capital of India is **New Delhi**.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… API keys loaded. Initializing Google Gemini model...\")\n",
    "\n",
    "# Initialize the Google Gemini model.\n",
    "# It automatically finds your GOOGLE_API_KEY from the .env file.\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
    "\n",
    "print(\"ðŸš€ Model ready. Asking question...\")\n",
    "\n",
    "question = \"What is the capital of India?\"\n",
    "response = llm.invoke(question)\n",
    "\n",
    "print(\"\\n--- Response ---\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fda513d",
   "metadata": {},
   "source": [
    "# Loading the models to local from the hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2613bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "\n",
    "# Use model_id instead of repo_id\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # <-- changed here\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_new_tokens\": 100\n",
    "    }\n",
    ")\n",
    "\n",
    "# Wrap in ChatHuggingFace\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# Run a query\n",
    "result = chat_model.invoke(\"What is the capital of India?\")\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010038cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/rgukt/data/LangChain/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5d47b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecae0d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "what is my name</s>\n",
      "<|assistant|>\n",
      "Yes, I can help you with that. Your name is [insert your name here].\n"
     ]
    }
   ],
   "source": [
    "result = chat_model.invoke(\"what is my name\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d43f5ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "who are you</s>\n",
      "<|assistant|>\n",
      "The author is not identifiable in the given text.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = chat_model.invoke(\"who are you\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d402654",
   "metadata": {},
   "source": [
    "if we dont use the proper english words model will fails\n",
    "so preprocessing the data is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fadb554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "who is the capital of india</s>\n",
      "<|assistant|>\n",
      "The capital of India is New Delhi.\n",
      "\n",
      "The capital of India is New Delhi, located in the northern part of the country. Delhi is India's largest city and is home to many of the country's government and administrative institutions. It is also the center of the Delhi Capital Region, which includes the cities of Gurgaon, Faridabad, and Noida. Delhi is an important economic and cultural center in India, and its location in the north\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "result = chat_model.invoke(\"who is the capital of india\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7163493c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "what is the capital of the India</s>\n",
      "<|assistant|>\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "result = chat_model.invoke(\"what is the capital of the India\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bef679",
   "metadata": {},
   "source": [
    "if we give correct prompt it will easily present answer in 2.4 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d46a9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79ed45ce",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
