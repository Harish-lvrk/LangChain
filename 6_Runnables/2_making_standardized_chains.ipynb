{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a6c3bba",
   "metadata": {},
   "source": [
    "Good question — this is exactly the context you need before appreciating **why Runnables were introduced in LangChain**. Let’s go step by step like proper class notes.\n",
    "\n",
    "---\n",
    "\n",
    "1. Situation Before Runnables\n",
    "\n",
    "In earlier versions of LangChain (before late 2023), different components had **different interfaces**:\n",
    "\n",
    "* `LLM` objects had `.predict()` or `.generate()`\n",
    "* `ChatModels` had `.predict_messages()` or `.predict()`\n",
    "* `PromptTemplate` used `.format()`\n",
    "* `OutputParsers` had `.parse()`\n",
    "* Retrievers used `.get_relevant_documents()`\n",
    "* Tools had `.run()` or `.arun()`\n",
    "\n",
    "Each component worked differently, with **no single standard method** to call them.\n",
    "\n",
    "---\n",
    "\n",
    "2. Problems Caused\n",
    "\n",
    "a) **Inconsistency**\n",
    "You had to remember many different method names (predict, generate, run, format, parse…). This made code harder to read, write, and maintain.\n",
    "\n",
    "b) **Difficult Composition**\n",
    "If you wanted to connect a retriever → prompt template → LLM → parser:\n",
    "\n",
    "* Each step had its own API.\n",
    "* You had to manually write “glue code” to pass outputs from one component into the next.\n",
    "\n",
    "c) **Async vs Sync Confusion**\n",
    "Some methods supported async (`arun`, `apredict`), some didn’t. Developers had to handle this manually, leading to messy code.\n",
    "\n",
    "d) **No Built-in Batching/Streaming**\n",
    "\n",
    "* If you wanted to run a model on a list of inputs, you had to loop yourself.\n",
    "* If you wanted streaming outputs, each model had its own streaming API (if any). Nothing unified.\n",
    "\n",
    "e) **Debugging / Observability Pain**\n",
    "Because there was no standard interface, logging, tracing, or monitoring required writing wrappers around every different component.\n",
    "\n",
    "---\n",
    "\n",
    "3. Why Standardization with Runnables\n",
    "\n",
    "LangChain created the **Runnable Protocol** to fix these issues:\n",
    "\n",
    "• One method (`.invoke`) to call any component (LLM, retriever, parser, tool, lambda, etc.).\n",
    "• Same API for synchronous, asynchronous, batched, and streaming execution.\n",
    "• Easy to compose: use `|` to chain steps instead of manually writing glue code.\n",
    "• Built-in hooks for observability (logging, tracing, callbacks).\n",
    "\n",
    "---\n",
    "\n",
    "4. Real-life Analogy\n",
    "\n",
    "Imagine before Runnables:\n",
    "\n",
    "* Every electrical appliance in your house had a **different plug** (TV plug, fan plug, fridge plug).\n",
    "* You’d need adapters for each one.\n",
    "* Messy, frustrating, and easy to mess up.\n",
    "\n",
    "Runnables = **standardized universal plug**.\n",
    "Now you just plug anything into the same socket, and it works the same way.\n",
    "\n",
    "---\n",
    "\n",
    "5. Summary\n",
    "\n",
    "Before Runnables:\n",
    "\n",
    "* Many inconsistent method names\n",
    "* Hard to compose chains\n",
    "* Async/batch/stream not unified\n",
    "* Debugging and monitoring messy\n",
    "\n",
    "After Runnables:\n",
    "\n",
    "* Single clean API (`invoke`, `batch`, `stream`)\n",
    "* Easy composition (`|`, `RunnableParallel`, `RunnableBranch`)\n",
    "* Unified async/batch/stream handling\n",
    "* Standardized observability\n",
    "\n",
    "That’s why Runnables are the foundation of modern LangChain.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to make a **side-by-side code comparison** (before vs after Runnables) so you see the difference clearly?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a5e217",
   "metadata": {},
   "source": [
    "## **how they standardized**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f873e",
   "metadata": {},
   "source": [
    "Perfect — let’s focus on **intuition**. Imagine I’m teaching this in class: *“Why Runnables feel natural and how they solve the old mess.”*\n",
    "\n",
    "---\n",
    "\n",
    "1. The Old World (Before Runnables) – Intuition\n",
    "\n",
    "Think of LangChain like a city before a good transport system:\n",
    "\n",
    "* Each street had its **own type of bus stop** (different shapes, signs, rules).\n",
    "* A bus from one street could not easily connect to another — you had to walk and find a new stop.\n",
    "* Some stops allowed **express buses** (async), some didn’t.\n",
    "* Some stops only allowed **one passenger at a time** (no batching).\n",
    "\n",
    "So, if you wanted to travel across the city (retriever → prompt → LLM → parser), you had to constantly switch stops, figure out the rules, and adapt. Messy and exhausting.\n",
    "\n",
    "---\n",
    "\n",
    "2. What Runnables Do – Intuitive Fix\n",
    "\n",
    "Now imagine the city builds a **unified metro system**:\n",
    "\n",
    "* Every station looks and works the same.\n",
    "* You tap your metro card once, and you can travel anywhere without worrying about local rules.\n",
    "* The same card works for single trips (invoke), group trips (batch), or continuous rides (stream).\n",
    "* You can connect stations in a line (sequence) or split into multiple lines at once (parallel).\n",
    "\n",
    "That’s exactly what Runnables did for LangChain.\n",
    "\n",
    "---\n",
    "\n",
    "3. Step-by-Step Intuition\n",
    "\n",
    "a) **Single Door (invoke)**\n",
    "Instead of remembering 10 different doors (`predict`, `run`, `format`…), now everything has one door: `.invoke(input)`.\n",
    "\n",
    "b) **Group Entry (batch)**\n",
    "Earlier, if you had 100 inputs, you had to push them one by one. Now, `.batch([...])` lets everyone enter at once.\n",
    "\n",
    "c) **Live Tour Guide (stream)**\n",
    "Before, you had to wait till the end to see results. Now, with `.stream()`, you hear updates as they happen (like a guide narrating live during the ride).\n",
    "\n",
    "d) **Pipe System (| operator)**\n",
    "Instead of manually handing over papers from one worker to another, you now have a conveyor belt: `retriever | prompt | LLM | parser`. Outputs automatically become inputs for the next.\n",
    "\n",
    "e) **Traffic Junctions (parallel & branch)**\n",
    "\n",
    "* Parallel = send the same car to multiple roads at once.\n",
    "* Branch = traffic cop directs cars to different roads based on rules.\n",
    "\n",
    "---\n",
    "\n",
    "4. Why It Feels Natural\n",
    "\n",
    "• One Mental Model: Instead of learning 10 APIs, you just learn “invoke, batch, stream, pipe.”\n",
    "• Lego-like Composition: Every piece fits into every other piece.\n",
    "• Less Friction: You focus on *what you want to build*, not on *how to connect pieces*.\n",
    "\n",
    "---\n",
    "\n",
    "5. Intuitive Summary\n",
    "\n",
    "Before Runnables → messy city with different bus stops and rules.\n",
    "After Runnables → clean metro system with one card, one interface, multiple options.\n",
    "\n",
    "So intuitively: **Runnables give you one universal plug (invoke) and a conveyor system (|) to connect everything seamlessly.**\n",
    "\n",
    "---\n",
    "\n",
    "Do you want me to now **show this intuition with a before-and-after code example** (like “old way” vs “Runnable way”) so it clicks both conceptually and practically?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e80583f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC,abstractmethod\n",
    "\n",
    "class Runnable(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def invoke(input_data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd5aee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "debaa6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class LLM(Runnable):\n",
    "    def __init__(self):\n",
    "        print(\"model instanciated\")\n",
    "\n",
    "    def invoke(self,prompt):\n",
    "        sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Python is a versatile programming language.\",\n",
    "    \"Lists are useful for storing collections of items.\",\n",
    "    \"This is another example sentence.\"\n",
    "    ]\n",
    "        return {'response':random.choice(sentences)}\n",
    "\n",
    "    def predict(self,prompt):\n",
    "       \"\"\" this method is going to be depricated soon \"\"\"\n",
    "       sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Python is a versatile programming language.\",\n",
    "    \"Lists are useful for storing collections of items.\",\n",
    "    \"This is another example sentence.\"\n",
    "    ]\n",
    "       return {'response':random.choice(sentences)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09ebce01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model instanciated\n"
     ]
    }
   ],
   "source": [
    "model = LLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f33dd988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': 'Lists are useful for storing collections of items.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"who is the director of the rgukt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d9c0f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptTemplate(Runnable):\n",
    "    def __init__(self,template,input_varibles):\n",
    "        self.template = template\n",
    "        self.input_varibles = input_varibles\n",
    "\n",
    "    def invoke(self,input_dict):\n",
    "        return self.template.format(**input_dict)\n",
    "    def format(self,input_dict):\n",
    "        \"\"\"This method is going to be depricated\"\"\"\n",
    "        return self.template.format(**input_dict)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "421a953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate(\n",
    "    template= \"tell me about the {topic} in a {mode}\",\n",
    "    input_varibles= ['topic','mode']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33265d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format({'topic':'rgukt','mode':'cool'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af4c493a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tell me about the rgukt in a cool'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4299b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': 'This is another example sentence.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(prompt= prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06245516",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2675d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NakliStrOutputParser(Runnable):\n",
    "\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def invoke(self, input_data):\n",
    "    return input_data['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ffebb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunnableConnector(Runnable):\n",
    "    def __init__(self,runnable_list):\n",
    "        self.runnable_list = runnable_list\n",
    "        \n",
    "    def invoke(self,input_data):\n",
    "        for runnable in self.runnable_list:\n",
    "            input_data = runnable.invoke(input_data)\n",
    "        return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b580b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate(\n",
    "    template= \"write a {length} poem about the {topic}\",\n",
    "    input_varibles= ['length','topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8cac264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model instanciated\n"
     ]
    }
   ],
   "source": [
    "model = LLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9740f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableConnector([template , model] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb28a1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': 'The quick brown fox jumps over the lazy dog.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'topic':\"rgukt\",'length':100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16811c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6ae979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b2309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a1aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94e585d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
